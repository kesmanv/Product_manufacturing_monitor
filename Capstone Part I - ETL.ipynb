{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project: Near real-time monitoring of a manufacturing production line\n",
    "**Keywords:** <font color='green'>SQL, AWS (S3 bucket, Sagemaker-XGBoost, lambda function, API Gateway), Power BI (Streaming dashboard, API)</font>   \n",
    "\n",
    "**Background:** The objective of this capstone project was to build and deploy a model to monitor in near real-time a customer-critical attribute of a product being manufactured at one of my company's facilities. For a variety of reasons, such limited resources and testing capabilities, this critical attribute can only be measured every 12 hours. Finished products are manufactured at 400-600 per minute rate. Therefore, a failure to meet this customer-critical attribute has the consequence of having to put on hold (often scrap) 12 hours of production. During these 12-hour periods several quality and production checks are performed at each stage of the product manufacturing process. The built model uses the data from these intermediary checks to predict the customer-critical attribute during the 12-hour intervals where this attribute is not measured directly. If a failure is predicted, a notification is sent to the appropriate personnel to take immediate action. \n",
    "#### Project structure:\n",
    "- **Part I: ETL** The data used for training, validation and testing is hosted on two SQL servers. One SQL server host product quality data and the other SQL server host the machine state data. The first step is to extract the data from the SQL Servers, transform it and load it to an AWS S3 bucket. \n",
    "- **Part II:** \n",
    " - **II.1 Build, Train and Deploy the model** With the data in AWS S3, I used Sagemaker to train and deploy an XGBoost model. Deploying the model creates an endpoint that can be accessed for predictions. \n",
    " - **II.2 Lambda function & Gateway API** I created a lambda function & GateWay API to be able to access the model for predictions. The API allows me to the send the data for prediction as a POST request for low-latency response. This is a cost effective solution since I am only charged when I send the request to the API. \n",
    "- **Part III: Predict near real-time and stream to PowerBI dashboard** With the model deployed and the API in service, I scheduled a taks on one of our on-premises servers to send the latest intermediary check to the model and get a prediction of the customer-critical attribute. This prediction is then pass to the PowerBI dashboard (also as POST request). The PowerBI Streaming dashboard visualizes the predictions in real-time.   \n",
    "\n",
    "### <font color='brown'>Part I:</font> Data transformation\n",
    "* After quering the SQL databases with the data for training and testing I performed data transformation and feature engineering. \n",
    "* The output is saved to csv file and uploaded to a AWS S3 bucket for training the XGBoost model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib as plt\n",
    "from sklearn import linear_model, ensemble\n",
    "import numpy as np\n",
    "# Setting pandas display options to show all rows\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to both the dat and nwh files\n",
    "filenames = ['CAN - AXIAL LOAD', \n",
    "            'CAN - BACKEND FINISHED CAN', \n",
    "            'CAN - BEAD DEPTH',\n",
    "            'CAN - PANEL RESISTANCE']\n",
    "data_path = 'Axial_load/300x407/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NWH Header files are XML files. I defined a function to parse the column names from the header file\n",
    "def xmlParse(filename):\n",
    "    path = ''.join([data_path, filename, '.NWH'])\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    col_names = []\n",
    "    for col in root.iter('ColumnParameter'):\n",
    "        col_names.append('_'.join([col[0].text,col[1].text]))\n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from DAT file\n",
    "data = []\n",
    "for name in filenames:\n",
    "    col_names = xmlParse(name) # Getting column names from NWH header file\n",
    "    path = ''.join([data_path, name, '.DAT'])\n",
    "    df = pd.read_csv(path, names=col_names, sep=' ')\n",
    "    data.append(df)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping columns with no information. It would be good to start collecting Supplier & Raw ID data\n",
    "for i in range(len(data)):\n",
    "    col_to_drop = ['FACTORY_A',\n",
    "                   'SET_NO_A',\n",
    "                   'DEPT_A',\n",
    "                   'GAGENAME_A',\n",
    "                   'ENGRAVE_A', # they seem to only run 'Stock'\n",
    "                   'SUPPLIER_A', # I want to collect this data to be able to use coil certificate data\n",
    "                   'RAW_ID_A',  # I want to collect this data to be able to use coil certificate data\n",
    "                   'NOTES_A',\n",
    "                   'FILENAME_A',\n",
    "                   'SPARE_1_A', \n",
    "                   'SPARE_2_A', \n",
    "                   'SPARE_3_A', \n",
    "                   'SPARE_4_A', \n",
    "                   'SAMPLESIZE_Z',\n",
    "                   'CHKTYPE_A', # Only using production data\n",
    "                   'SPECIFICATION_A', # It is only 1 spec\n",
    "                   'CUSTOMER_A' # Everything is 'Stock'\n",
    "                  ]\n",
    "    data[i].drop(columns=col_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial = data[0]\n",
    "backend = data[1].drop(columns='CHECKNO_A')\n",
    "bead = data[2].drop(columns=['ZLAST_I', 'QMDSTATION_N']) # 1 check per shift\n",
    "panel = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One check per shift\n",
    "axial.rename(columns = {'TIME_T': 'TIME_Axial', 'OPER_A': 'OPER_Axial'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 checks per shift\n",
    "backend.rename(columns = {'TIME_T': 'TIME_BE', 'OPER_BE': 'OPER_BE'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on_cols = ['DATE_D', 'CREW_A', 'LINE_A', 'MACHINE_A', 'STATION_A', 'MS_NUMBER_A']\n",
    "df = pd.merge(backend, axial, how='left', on=merge_on_cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bead.rename(columns={'TIME_T':'TIME_Bead','OPER_A':'OPER_Bead', 'FB_BEADER_TEMPERATURE_I': 'BEADER_TMP'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on_cols = ['DATE_D', 'CREW_A', 'LINE_A', 'MACHINE_A', 'STATION_A', 'MS_NUMBER_A']\n",
    "df = pd.merge(df, bead, how='left', on=merge_on_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel.rename(columns={'TIME_T':'TIME_panel', 'OPER_A':'OPER_bead'}, inplace=True)\n",
    "panel.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_on_cols = ['DATE_D', 'CREW_A', 'LINE_A', 'MACHINE_A', 'STATION_A', 'MS_NUMBER_A', 'FB_BODY_MAKER_A']\n",
    "df = pd.merge(df, panel, how='left', on=merge_on_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_features = ['FBA__I', \n",
    "                 'FBH_AVG_I', \n",
    "                 'FBE_AVG_I', \n",
    "                 'FBD_AVG_I', \n",
    "                 'BEADER_TMP', \n",
    "                 'FBL_AVG_I', \n",
    "                 'FBL1AVG_I',\n",
    "                'FBL2AVG_I',\n",
    "                'FBL3AVG_I',\n",
    "                 'FBL4AVG_I',\n",
    "                 'FBL5AVG_I',\n",
    "                 'FBL6AVG_I',\n",
    "                 'FBL7AVG_I',\n",
    "                 'FBL8AVG_I',\n",
    "                 'FBL9AVG_I',\n",
    "                 'FBL10AVG_I',\n",
    "                 'FBL11AVG_I',\n",
    "                 'FBL12AVG_I',\n",
    "                 'FBL13AVG_I',\n",
    "                 'FBL14AVG_I',\n",
    "                 'FBL15AVG_I',\n",
    "                 'FBL16AVG_I',\n",
    "                 'FBL17AVG_I',\n",
    "                 'FBL18AVG_I',\n",
    "                 'FBL19AVG_I',\n",
    "                 'FBAXIAL_I'\n",
    "                 \n",
    "                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('*', np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features with NA values\n",
    "df.isnull().sum().sort_values(ascending=False).reset_index().rename(columns={0:'Feature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping columns with mostly NAs\n",
    "df.drop(['OPER_bead', \n",
    "               'FBPANEL_I',\n",
    "               'TIME_panel', \n",
    "               'CHECKNO_A_y',\n",
    "               'CHECKNO_A', \n",
    "               'FBL19AVG_I',\n",
    "               'FBL19_1', \n",
    "               'FBL19_2', \n",
    "               'FBL19_3', \n",
    "               'FBL19RNG_I'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping rows with NAs\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['DATE_D', 'TIME_BE', 'TIME_Axial', 'TIME_Bead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data to file\n",
    "df.to_csv('./Axial_load/300x407/axial_for_training.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
